{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI PROJECT_3 SARA TAVAKOLI 810196684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "موضوع پروژه : پردازش متن و شبکه های بیزین\n",
    "در این پروژه می خواهیم یک سیستم دسته بندی موضوع اخبار بسازیم و به کمک آن موضوع خبر هایی که به ما داده شده است را تعیین کنیم. برای این کار ابتدا دسته ای از داده هایی که به ما داده شده است (فایل دیتا) استفاده می کنیم تا به کمک آن ها اخبار های موجود در هر دسته را کاوش کنیم و احتمال هر کلمه در آن اخبار را به کمک شبکه بیزین به دست آوریم و سیستم خود را ترین کنیم. \n",
    "برای ارزیابی این سیستم از بخش دیگری از داده ها استفاده می کنیم تا میزان درستی حدس خود را بسنجیم. و در نهایت سیستم ساخته شده را روی داده های تست اعمال می کنیم و دسته ی آن ها را مشخص می کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "FEILD = 'short_description'\n",
    "FILELOCATION = \"Attachment/data.csv\"\n",
    "TESTFILELOCATION = \"Attachment/test.csv\"\n",
    "CATEGORYCOL = 'category'\n",
    "BUSINESSCATEGORY = 'BUSINESS'\n",
    "STYLEANDBEAUTYCATEGORY = 'STYLE & BEAUTY'\n",
    "TRAVELCATEGORY = 'TRAVEL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تمیز کردن داده:\n",
    "ابتدا باید متن مربوط به هر خبر را تمیز کنیم.\n",
    "برای این کار مراحل زیر را انجام می دهیم:\n",
    "nanابتدا سطر هایی که ستون مورد نظر آن ها \n",
    "باشد را حذف  میکنیم\n",
    "\n",
    "سپس تمام حروف متن را به حروف کوچک تبدیل می کنیم. زیرا بزرگ بودن یک حرف در کلمه تباید باعث شود که آن را متمایز از حالتی بگیریم که یک یا چند حرف در آن کلمه بزرگ باشند\n",
    "\n",
    "سپس کلماتی مانند ضمیر های اشاره و شمیر های شخص و ... را جذف می کنیم زیرا این کلمات در تشخیص نوع خبر به ما کمکی نمیکنند و تفاوتی ایجاد نمی شود.\n",
    "\n",
    "سپس تمام علائم نگارشی و اعداد را نیز به دلیل مفید نبودن و بی تاثیر بودن در تشخیض حذف می کنیم\n",
    "\n",
    "بعد از آن با استفاده از روش stemming\n",
    "کلمات را با ریشه ه آن ها جایگزین می کنیم تا به ازای همه ی آن ها یک کلمه داشته باشیم. زیرا در واقع اگر هر حالتی از یک فعل در متن آمده باشد در واقع بیان گر ریشه همان فعل است و نیازس به داشتن تمام آن حالت ها در دیکشنری هر دسته خبر نداریم.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileLoc):\n",
    "    return pd.read_csv(fileLoc)\n",
    "\n",
    "def removeRowsWithNanDescription(data, textField):\n",
    "    data = data[data['short_description'].notnull()]\n",
    "    return data\n",
    "\n",
    "def changeUpperCaseCharsWithLower(data, textField):\n",
    "    data[textField] = data[textField].str.lower()\n",
    "    return data\n",
    "\n",
    "def removeSymbolsAnsNums(data, textField):\n",
    "    data[textField] = data[textField].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    data[textField] = data[textField].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    return data\n",
    "\n",
    "def cleanText(data, textField):\n",
    "    data = changeUpperCaseCharsWithLower(data, textField)\n",
    "    data = removeSymbolsAnsNums(data, textField)\n",
    "    return data\n",
    "\n",
    "def removeStopWords(data, textField):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    data[textField] = data[textField].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    return data\n",
    "\n",
    "def tokenise(data, textField):\n",
    "    nltk.download('punkt')\n",
    "    data[textField] = data[textField].apply(lambda x: nltk.tokenize.word_tokenize(x))\n",
    "    return data\n",
    "\n",
    "def wordStemmer(text):\n",
    "    stem_text = [nltk.stem.PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "\n",
    "def stemmingText(data, textField):\n",
    "    data[textField] = data[textField].apply(lambda x: wordStemmer(x))\n",
    "    return data\n",
    "\n",
    "def normalizeData(fileLoc, textField):\n",
    "    data = readData(fileLoc)\n",
    "    data = removeRowsWithNanDescription(data, textField)\n",
    "    data = changeUpperCaseCharsWithLower(data, textField)\n",
    "    data = removeStopWords(data, textField)\n",
    "    data = removeSymbolsAnsNums(data, textField)\n",
    "    data = tokenise(data, textField)\n",
    "    data = stemmingText(data, textField)\n",
    "    # data = lemmatizeText(data, textField)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "قاعده بیزین:\n",
    "$$ P(C\\mid X) = \\frac{P(X \\mid C) \\, P(C)}{P(X)} $$\n",
    "\n",
    "Posterior P(c|x): به چه احتمالی یک خبر به این کتگوری تعلق دارد. برای هر خبر به ازای هر کتگوری چک میکنیم که این خبر با چه احتمالی در آن کتگوری قرار دارد. (به کمک محاسبه ی احتمال وجود کلمات خبر در آن کتگوری)\n",
    "قرار دارد a در کتگوری x یعنی  با چه احتمالی خبر \n",
    "\n",
    "Likelihood P(x|c): است. که برای محاسبه آن به ازای هر کلمه در خبر احتمال آن کلمه a با چه احتمالی شامل خبر c یعنی کتگوری\n",
    "در کتگوری را در احتمال حساب شده تا آن مرحله ضرب می کنیم.\n",
    "یعنی احتمال وجود کلمات آن خبر در کتگوری\n",
    "\n",
    "Class Prior P(c): احتمال آن کتگوری. یعنی احتمال وقوع آن کتگوری چقدر است\n",
    "برای محاسبه تعداد کلمات آن کتگوری را به تعداد کلمات کل داده ترین تقسیم می کنیم.\n",
    "\n",
    "Evidence P(x): است. که چون می خواهیم احتمالات به ازای کتگوری های مختلف را   x  اویدنس رخ داده درواقع احتمال رخ دادن خبر\n",
    "مقایسه کنیم و این مخرج در تمام آن ها یکی است آن را محاسبه نمیکنیم\n",
    "\n",
    "در نهایت برای محاسبه \n",
    "ابتدا باید احتمال رخداد کلمه های خبر در کتگوری را در هم ضرب می کنیم و هم چنین در احتمال آن کتگوری ضرب کنیم. likelihood \n",
    "چون عدد حاصل بسیار کوچک میشود از آن لگاریتم میگیریم و به جمع تبدیل می شود. بنابراین باید لگاریتم ها را با هم جمع کنیم\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "برای ترین کردن داده ها ۸۰ درصد از هر کتگوری را به صورت رندم انتخاب کرده و به هم کانکت می کنیم. بعد ۲۰ درصد باقی مانده را برای ارزیابی ذخیره میکنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainigAndTestingData(fileLoc, fieldToNormalize):\n",
    "    data = normalizeData(fileLoc, fieldToNormalize)\n",
    "    businessCategory = data.loc[data[CATEGORYCOL] == BUSINESSCATEGORY]\n",
    "    styleAndBeautyCategory = data.loc[data[CATEGORYCOL] == STYLEANDBEAUTYCATEGORY]\n",
    "    travelCategory = data.loc[data[CATEGORYCOL] == TRAVELCATEGORY]\n",
    "    trainingData = pd.concat([businessCategory.sample(frac = 0.8), styleAndBeautyCategory.sample(frac = 0.8), travelCategory.sample(frac = 0.8)])\n",
    "    testingData = data.drop(trainingData.index)\n",
    "    return trainingData, testingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بعد از آماده کردن دیتا ی ترین آن را بنابر کتگوری به ۳ دسته تقسیم می کنیم و برای هر دسته احتمال تکرار هر کلمه را محاسبه میکنیم. با فرمول تقسیم کردن تعداد تکرار های آن کلمه بر تعداد کل کلمات آن کتگوری.\n",
    "\n",
    "بنابراین کلاس کتگوری را میسازیم و احتمال کلمات و طول کل کلمات و مجموعه کلمات و احتمال رخداد آن کتگوری در دیتای ترین \n",
    "و ... را به ازای هر کتگوری ذخیره میکنیم.\n",
    "\n",
    "برای حالتی که کلمه ی موجود در خبر در کلمات کتگوری نباشد نحوه محاسبه در سوال ۴ آورده شده\n",
    "و به همین دلیل برای کلماتی که در کتگوری وجود دارند تعداد را از ۲ شروع کردیم که در واقع همه را با یک جمع کرده باشیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    def __init__(self, categoyName):\n",
    "        self.name = categoyName\n",
    "        self.accuracyPhase1 = 0\n",
    "        self.precisionPhase1 = 0\n",
    "        self.recallPhase1 = 0\n",
    "        self.accuracyPhase2 = 0\n",
    "        self.precisionPhase2 = 0\n",
    "        self.recallPhase2 = 0\n",
    "        self.p = 0\n",
    "        \n",
    "    def calculateProbabilityOfWords(self, data):\n",
    "        self.wordsProbability = {}\n",
    "        wordsNum = {}\n",
    "        lenOfDescription = data[FEILD].apply(lambda x: len(x))\n",
    "        s = 0\n",
    "        for num in lenOfDescription:\n",
    "            s += num\n",
    "        self.lenOfAllWords = s\n",
    "        self.allWords = set()\n",
    "        for arr in data[FEILD]:\n",
    "            for word in arr:\n",
    "                if word not in self.allWords:\n",
    "                    wordsNum[word] = 2\n",
    "                    self.allWords.add(word)\n",
    "                else:\n",
    "                    wordsNum[word] += 1\n",
    "\n",
    "        for x in wordsNum:\n",
    "            self.wordsProbability[x] = round(wordsNum[x]*100/self.lenOfAllWords, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به ازای تمام خبر های موجود در داده ی تست احتمال آن خبر در تمام کتگوری ها را محاسبه می کنیم و بین آن ها ماکسیمم می گیریم و کتگوری حدس زده شده را در ستونی جدید با نام \n",
    "GUESS\n",
    "ذخیره می کنیم\n",
    "برای هر دو فاز به صورت جدا\n",
    "زیرا برای فاز اول باید سطر هایی که کتگوری استایل دارند را حذف کنیم. در غیر این صورت محاسبات مربوط به بخش \n",
    "confusion matrix\n",
    "غلط خواهد شد و بازده کم می شود"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCategory(words, categories):\n",
    "    probabilities = {}\n",
    "    s = 0\n",
    "    for c in categories:\n",
    "        s += c.lenOfAllWords\n",
    "    for c in categories:\n",
    "        c.p = round((c.lenOfAllWords * 100/s), 8)\n",
    "    for category in categories:\n",
    "        probabilityForWordsNotExistInCategory = round(100/category.lenOfAllWords, 8)\n",
    "        probabilities[category.name] = math.log10(category.p)\n",
    "        for word in words:\n",
    "            if word in category.allWords:\n",
    "                probabilities[category.name] += math.log10(category.wordsProbability[word])\n",
    "            else:\n",
    "                probabilities[category.name] += math.log10(probabilityForWordsNotExistInCategory)\n",
    "    chosenCategory = ''\n",
    "    maximum = -sys.maxsize - 1\n",
    "    for p in probabilities:\n",
    "        if probabilities[p] > maximum:\n",
    "            maximum = probabilities[p]\n",
    "            chosenCategory = p\n",
    "    return chosenCategory\n",
    "\n",
    "def predictForAllNewsPhase1():\n",
    "    categoriesPhase1, categoriesPhase2, testingData = calculateProbabilityOfEachWordInCategorys(FEILD, False, False)\n",
    "    testingData = testingData[~(testingData[CATEGORYCOL] == STYLEANDBEAUTYCATEGORY)]\n",
    "    testingData['GUESS'] = testingData[FEILD].apply(lambda x: predictCategory(x, categoriesPhase1))\n",
    "    return categoriesPhase1, categoriesPhase2, testingData\n",
    "\n",
    "def predictForAllNewsPhase2():\n",
    "    categoriesPhase1, categoriesPhase2, testingData = calculateProbabilityOfEachWordInCategorys(FEILD, False, False)\n",
    "    testingData['GUESS'] = testingData[FEILD].apply(lambda x: predictCategory(x, categoriesPhase2))\n",
    "    return categoriesPhase2, testingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion matrix is one of the most intuitive and easiest (unless of course, you are not confused)metrics used for finding the correctness and accuracy of the model. It is used for Classification problem where the output can be of two or more types of classes.\n",
    "\n",
    "Before diving into what the confusion matrix is all about and what it conveys, Let’s say we are solving a classification problem where we are predicting whether a person is having cancer or not.\n",
    "\n",
    "Let’s give a label of to our target variable:\n",
    "1: When a person is having cancer 0: When a person is NOT having cancer.\n",
    "Terms associated with Confusion matrix:\n",
    "\n",
    "1. True Positives (TP): True positives are the cases when the actual class of the data point was 1(True) and the predicted is also 1(True)\n",
    "\n",
    "Ex: The case where a person is actually having cancer(1) and the model classifying his case as cancer(1) comes under True positive.\n",
    "\n",
    "2. True Negatives (TN): True negatives are the cases when the actual class of the data point was 0(False) and the predicted is also 0(False\n",
    "\n",
    "Ex: The case where a person NOT having cancer and the model classifying his case as Not cancer comes under True Negatives.\n",
    "\n",
    "3. False Positives (FP): False positives are the cases when the actual class of the data point was 0(False) and the predicted is 1(True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one. (1)\n",
    "\n",
    "Ex: A person NOT having cancer and the model classifying his case as cancer comes under False Positives.\n",
    "\n",
    "4. False Negatives (FN): False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). False is because the model has predicted incorrectly and negative because the class predicted was a negative one. (0)\n",
    "\n",
    "Ex: A person having cancer and the model classifying his case as No-cancer comes under False Negatives.\n",
    "\n",
    "\n",
    "source : https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به ازای هر دو فاز با فرمول های زیر داریم:\n",
    "\n",
    "$$ ACCURACY = \\frac{TP + TN}{TP + FP + FN + PN} $$\n",
    "\n",
    "$$ PRECISION = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "$$ RECALL = \\frac{TP}{TP + FN} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluationPhase1():\n",
    "    categoriesPhase1, categoriesPhase2, predictedData = predictForAllNewsPhase1()\n",
    "    for category in categoriesPhase1:\n",
    "        tp = len(predictedData[(predictedData[CATEGORYCOL] == category.name) & (predictedData[\"GUESS\"] == category.name)])\n",
    "        tn = len(predictedData[(predictedData[CATEGORYCOL] != category.name) & (predictedData[\"GUESS\"] != category.name)])\n",
    "        fp = len(predictedData[(predictedData[CATEGORYCOL] != category.name) & (predictedData[\"GUESS\"] == category.name)])\n",
    "        fn = len(predictedData[(predictedData[CATEGORYCOL] == category.name) & (predictedData[\"GUESS\"] != category.name)])\n",
    "        category.accuracyPhase1 = (tp + tn) / (tp + tn + fp + fn)\n",
    "        category.precisionPhase1 = tp / (tp + fp)\n",
    "        category.recallPhase1 = tp / (tp + fn)\n",
    "    return categoriesPhase1\n",
    "\n",
    "def predictForAllNewsPhase2():\n",
    "    categoriesPhase1, categoriesPhase2, testingData = calculateProbabilityOfEachWordInCategorys(FEILD, False, False)\n",
    "    testingData['GUESS'] = testingData[FEILD].apply(lambda x: predictCategory(x, categoriesPhase2))\n",
    "    return categoriesPhase2, testingData\n",
    "\n",
    "def evaluationPhase2():\n",
    "    categoriesPhase2, predictedData = predictForAllNewsPhase2()\n",
    "    for category in categoriesPhase2:\n",
    "        tp = len(predictedData[(predictedData[CATEGORYCOL] == category.name) & (predictedData[\"GUESS\"] == category.name)])\n",
    "        tn = len(predictedData[(predictedData[CATEGORYCOL] != category.name) & (predictedData[\"GUESS\"] != category.name)])\n",
    "        fp = len(predictedData[(predictedData[CATEGORYCOL] != category.name) & (predictedData[\"GUESS\"] == category.name)])\n",
    "        fn = len(predictedData[(predictedData[CATEGORYCOL] == category.name) & (predictedData[\"GUESS\"] != category.name)])\n",
    "        category.accuracyPhase2 = (tp + tn) / (tp + tn + fp + fn)\n",
    "        category.precisionPhase2 = tp / (tp + fp)\n",
    "        category.recallPhase2 = tp / (tp + fn)\n",
    "    return categoriesPhase2\n",
    "\n",
    "categoryPhase1 = evaluationPhase1()\n",
    "for c in categoryPhase1:\n",
    "    print(c.name)\n",
    "    print(\"accuracy \", c.accuracyPhase1)\n",
    "    print(\"precision \", c.precisionPhase1)\n",
    "    print(\"recall \", c.recallPhase1)\n",
    "\n",
    "categoryFinalPhase2 = evaluationPhase2()\n",
    "for c in categoryFinalPhase2:\n",
    "    print(c.name)\n",
    "    print(\"accuracy \", c.accuracyPhase2)\n",
    "    print(\"precision \", c.precisionPhase2)\n",
    "    print(\"recall \", c.recallPhase2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\">\n",
    "    <tr></tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <table style=\"width:100%; border: 1px solid #ddd;\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th style=\"text-align: center;\">Phase #1</th>\n",
    "                    </tr>\n",
    "                <tr>\n",
    "                    <th style=\"text-align: center;\">Phase 1</th>\n",
    "                    <th style=\"text-align: center;\">Travel</th>\n",
    "                    <th style=\"text-align: center;\">Business</th>\n",
    "                </tr>\n",
    "                </thead>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: center;\"><b>Recall</b></td>         \n",
    "                    <td style=\"text-align: center;\">0.8676122931442081</td>\n",
    "                    <td style=\"text-align: center;\">0.9135667396061269</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: center;\"><b>Precision</b></td>        \n",
    "                    <td style=\"text-align: center;\">0.9489334195216548</td>\n",
    "                    <td style=\"text-align: center;\">0.7884796978281398</td>\n",
    "                </tr>        \n",
    "                <tr>\n",
    "                    <td style=\"text-align: center;\"><b>Accuracy</b></td>         \n",
    "                    <td style=\"text-align: center;\">0.8837298541826554</td>\n",
    "                    <td style=\"text-align: center;\">0.8837298541826554</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "        <td>\n",
    "            <table style=\"width:100%; border: 1px solid #ddd;\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th style=\"text-align: center;\">Phase #2</th>\n",
    "                    </tr>\n",
    "                <tr>\n",
    "                    <th style=\"text-align: center;\">Phase 2</th>\n",
    "                    <th style=\"text-align: center;\">Travel</th>\n",
    "                    <th style=\"text-align: center;\">Business</th>\n",
    "                    <th style=\"text-align: center;\">Style & Beauty</th>\n",
    "                </tr>\n",
    "                </thead>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: center;\"><b>Recall</b></td>         \n",
    "                    <td style=\"text-align: center;\">0.83392434988179</td>\n",
    "                    <td style=\"text-align: center;\">0.88402625820568</td>\n",
    "                    <td style=\"text-align: center;\">0.84380403458213</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: center;\"><b>Precision</b></td>        \n",
    "                    <td style=\"text-align: center;\">0.88242651657285</td>\n",
    "                    <td style=\"text-align: center;\">0.70017331022530</td>\n",
    "                    <td style=\"text-align: center;\">0.92191435768261</td>\n",
    "                </tr>        \n",
    "                <tr>\n",
    "                    <td style=\"text-align: center;\"><b>Accuracy</b></td>         \n",
    "                    <td style=\"text-align: center;\">0.89196037779313</td>\n",
    "                    <td style=\"text-align: center;\">0.89587652614604</td>\n",
    "                    <td style=\"text-align: center;\">0.90900714121170</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix for phase 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"f.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "برای حل مشکل \n",
    "oversampling:\n",
    "وقتی از هر کتگوری ۸۰ درصد را برمیداریم و با هم کانکت می کنیم و داده ی ترین را می سازیم باعث می شود که تعداد خبر های هر دسته با هم متفاوت باشند. بنابراین باعث می شود که در محاسبه ی احتمالات تعداد کلمات هر دسته شانس برابری نداشته باشند (یعنی یک دسته به دلیل زیاد بودن تعداد خبر هایش احتمال کلماتش از بقیه دسته ها بیشتر می شود.) بنابر این اندازه ی تمام دسته ها را به بزرگ ترین دسته می رسانیم.\n",
    "برای این کار از بین خبر های هر دسته به صورت رندم انتخاب می کنیم و با خودش کانکت می کنیم و در واقع در خودش کپی می کنیم. و با این روش باعث میشویم که تفاوت در \n",
    "precision and recall\n",
    "کم شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataForOversamplingPase1(fileLoc, fieldToNormalize):\n",
    "    data = normalizeData(fileLoc, fieldToNormalize)\n",
    "    businessCategory = data.loc[data[CATEGORYCOL] == BUSINESSCATEGORY]\n",
    "    travelCategory = data.loc[data[CATEGORYCOL] == TRAVELCATEGORY]\n",
    "    businessCategory = businessCategory.sample(frac = 0.8)\n",
    "    travelCategory = travelCategory.sample(frac = 0.8)\n",
    "    difference = abs(len(businessCategory) - len(travelCategory))\n",
    "    business = False\n",
    "    travel = False\n",
    "    if len(businessCategory) < len(travelCategory):\n",
    "        lowerOne = businessCategory\n",
    "        business = True\n",
    "    else:\n",
    "        lowerOne = travelCategory\n",
    "        travel = True\n",
    "    indexes = lowerOne.index\n",
    "    temp = lowerOne\n",
    "    while difference > 0:\n",
    "        rand = random.choice(indexes)\n",
    "        row = temp.loc[[rand]]\n",
    "        lowerOne = pd.concat([lowerOne, row])\n",
    "        difference-=1\n",
    "    if business:\n",
    "        businessCategory = lowerOne\n",
    "    elif travel:\n",
    "        travelCategory = lowerOne\n",
    "    trainingData = pd.concat([businessCategory, travelCategory])    \n",
    "    testingData = data.drop(trainingData.index)\n",
    "    testingData = testingData[~(testingData[CATEGORYCOL] == STYLEANDBEAUTYCATEGORY)]\n",
    "    return trainingData, testingData\n",
    "\n",
    "def oversamplingPase1():\n",
    "    categoriesPhase1, categoriesPhase2, testingData = calculateProbabilityOfEachWordInCategorys(FEILD, True, False)\n",
    "    testingData['GUESS'] = testingData[FEILD].apply(lambda x: predictCategory(x, categoriesPhase1))\n",
    "    for category in categoriesPhase1:\n",
    "        tp = len(testingData[(testingData[CATEGORYCOL] == category.name) & (testingData[\"GUESS\"] == category.name)])\n",
    "        tn = len(testingData[(testingData[CATEGORYCOL] != category.name) & (testingData[\"GUESS\"] != category.name)])\n",
    "        fp = len(testingData[(testingData[CATEGORYCOL] != category.name) & (testingData[\"GUESS\"] == category.name)])\n",
    "        fn = len(testingData[(testingData[CATEGORYCOL] == category.name) & (testingData[\"GUESS\"] != category.name)])\n",
    "        category.accuracyPhase1 = (tp + tn) / (tp + tn + fp + fn)\n",
    "        category.precisionPhase1 = tp / (tp + fp)\n",
    "        category.recallPhase1 = tp / (tp + fn)\n",
    "    return categoriesPhase1\n",
    "\n",
    "categoryPhase1 = oversamplingPase1()\n",
    "print(\"oversampling phase 1\")\n",
    "for c in categoryPhase1:\n",
    "    print(c.name)\n",
    "    print(\"accuracy \", c.accuracyPhase1)\n",
    "    print(\"precision \", c.precisionPhase1)\n",
    "    print(\"recall \", c.recallPhase1)\n",
    "\n",
    "def dataForOversamplingPase2(fileLoc, fieldToNormalize):\n",
    "    data = normalizeData(fileLoc, fieldToNormalize)\n",
    "    businessCategory = data.loc[data[CATEGORYCOL] == BUSINESSCATEGORY]\n",
    "    travelCategory = data.loc[data[CATEGORYCOL] == TRAVELCATEGORY]\n",
    "    styleAndBeautyCategory = data.loc[data[CATEGORYCOL] == STYLEANDBEAUTYCATEGORY]\n",
    "    businessCategory = businessCategory.sample(frac = 0.8)\n",
    "    travelCategory = travelCategory.sample(frac = 0.8)\n",
    "    styleAndBeautyCategory = styleAndBeautyCategory.sample(frac = 0.8)\n",
    "    business = True\n",
    "    travel = True\n",
    "    style = True\n",
    "    lenArr = [len(businessCategory), len(travelCategory), len(styleAndBeautyCategory)]\n",
    "    maxLen = max(lenArr)\n",
    "    smallerCategories = []\n",
    "    if len(businessCategory) != maxLen:\n",
    "        business = False\n",
    "        smallerCategories.append(businessCategory)\n",
    "    if len(travelCategory) != maxLen:\n",
    "        travel = False\n",
    "        smallerCategories.append(travelCategory)\n",
    "    if len(styleAndBeautyCategory) != maxLen:\n",
    "        style = False\n",
    "        smallerCategories.append(styleAndBeautyCategory)\n",
    "    if business:\n",
    "        trainingData = businessCategory\n",
    "    if travel:\n",
    "        trainingData = travelCategory\n",
    "    if style:\n",
    "        trainingData = styleAndBeautyCategory\n",
    "    for category in smallerCategories:\n",
    "        indexes = category.index\n",
    "        temp = category\n",
    "        while len(category) < maxLen:\n",
    "            rand = random.choice(indexes)\n",
    "            row = temp.loc[[rand]]\n",
    "            category = pd.concat([category, row])\n",
    "        trainingData = pd.concat([trainingData, category])\n",
    "    \n",
    "    testingData = data.drop(trainingData.index)\n",
    "    print(len(testingData))\n",
    "    return trainingData, testingData\n",
    "\n",
    "def oversamplingPase2():\n",
    "    categoriesPhase1, categoriesPhase2, testingData = calculateProbabilityOfEachWordInCategorys(FEILD, False, True)\n",
    "    testingData['GUESS'] = testingData[FEILD].apply(lambda x: predictCategory(x, categoriesPhase2))\n",
    "    for category in categoriesPhase2:\n",
    "        print(category.name)\n",
    "        tp = len(testingData[(testingData[CATEGORYCOL] == category.name) & (testingData[\"GUESS\"] == category.name)])\n",
    "        tn = len(testingData[(testingData[CATEGORYCOL] != category.name) & (testingData[\"GUESS\"] != category.name)])\n",
    "        fp = len(testingData[(testingData[CATEGORYCOL] != category.name) & (testingData[\"GUESS\"] == category.name)])\n",
    "        fn = len(testingData[(testingData[CATEGORYCOL] == category.name) & (testingData[\"GUESS\"] != category.name)])\n",
    "        category.accuracyPhase2 = (tp + tn) / (tp + tn + fp + fn)\n",
    "        category.precisionPhase2 = tp / (tp + fp)\n",
    "        category.recallPhase2 = tp / (tp + fn)\n",
    "    return categoriesPhase2\n",
    "\n",
    "categoryPhase2 = oversamplingPase2()\n",
    "print(\"oversampling phase 2\")\n",
    "for c in categoryPhase2:\n",
    "    print(c.name)\n",
    "    print(\"accuracy \", c.accuracyPhase2)\n",
    "    print(\"precision \", c.precisionPhase2)\n",
    "    print(\"recall \", c.recallPhase2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"g.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"h.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در مرحله آخر برای خبر های موجود در فایل تست نوع آن ها را حدس میزنیم و در یک فایل جدید نتیجه را ذخیره می کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(categoryFinalPhase2):\n",
    "    data = normalizeData(TESTFILELOCATION, FEILD)\n",
    "    data['category'] = data[FEILD].apply(lambda x: predictCategory(x, categoryFinalPhase2))\n",
    "    data = data[['index', 'category']]\n",
    "    data.drop(data.columns[1], axis=1)\n",
    "    data.to_csv('out.csv')\n",
    "    print(data, file = out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "درنرمال کردن با لمتایز فقط باید تابع مورد استفاده در نرمال کردن خبر ها را تغییر دهیم. اگر این کار را انجام دهیم داریم :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk2wnTag(nltkTag):\n",
    "    if nltkTag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif nltkTag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif nltkTag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif nltkTag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "\n",
    "def wordLemmatizer(text):\n",
    "    nltkTagged = nltk.pos_tag(nltk.word_tokenize(text))    \n",
    "    wnTagged = map(lambda x: (x[0], nltk2wnTag(x[1])), nltkTagged)\n",
    "\n",
    "    resWords = []\n",
    "    for word, tag in wnTagged:\n",
    "        if tag is None:                        \n",
    "            resWords.append(word)\n",
    "        else:\n",
    "            resWords.append(nltk.stem.WordNetLemmatizer().lemmatize(word, tag))\n",
    "\n",
    "    return resWords\n",
    "\n",
    "def lemmatizeText(data, textField):\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    data[textField] = data[textField].apply(lambda x: wordLemmatizer(x))\n",
    "    return data\n",
    "\n",
    "def normalizeData(fileLoc, textField):\n",
    "    data = readData(fileLoc)\n",
    "    data = removeRowsWithNanDescription(data, textField)\n",
    "    data = changeUpperCaseCharsWithLower(data, textField)\n",
    "    data = removeStopWords(data, textField)\n",
    "    data = removeSymbolsAnsNums(data, textField)\n",
    "    data = lemmatizeText(data, textField)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"w.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سوالات:\n",
    "\n",
    "1- Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech.\n",
    "\n",
    "For instance:\n",
    "\n",
    "1-    The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "2-    The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "\n",
    "3-    The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.\n",
    "\n",
    "\n",
    "در نتیجه استمینگ سریعتره و بیاده سازی آن راحت تر است ولی لزوما کلمه های آن معنی دار نیستند و فقط به ریشه بر می گرداند. ولی لمتایز همه کلمه هاش معنی دارن و کند تره.\n",
    "لمتایز با توجه به متن تصمیم میگیره و همچنین قابلیت تمیم گیری برای اینکه چه نقش کلماتی رو تغییر بده رو داره\n",
    "\n",
    "source :\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python and https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n",
    "\n",
    "2-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "TF-IDF was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    "\n",
    "However, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. For example, if what we’re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic.\n",
    "\n",
    "TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
    "\n",
    "1-    The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n",
    "\n",
    "2-    The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    "\n",
    "So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n",
    "\n",
    "<img src=\"tf.png\">\n",
    "\n",
    "\n",
    "در واقع در این روش هر داکیومنت مثل یک کتگوری در مساله ماست. و به ازای هر کلمه تعداد تکرار آن را با یک جمع می کند و لگاریتم میگیرد. که این معادل لایکلیهود در روش ماست و همچنین بارامتر دومی که در آن ضرب کرده هم لگاریتم احتمال کتگوری است که محاسبه کردیم\n",
    "\n",
    "بنابراین گر بخواهیم از این روش در بیزین استفاده کنیم باید متریک اول را به جای لایکلیهود و متریک دوم را به جای \n",
    "class perior \n",
    "قرار دهیم\n",
    "\n",
    "\n",
    "source : https://monkeylearn.com/blog/what-is-tf-idf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-\n",
    "به عنوان مثال یک بیمارستان را در نظر بگیرید که در تشخیص سرماخوردگی خیلی خوب است و چون تعداد کسانی که سرما خورده اند زیاد است بس کیس های \n",
    "زیاد است. در مقابل نمیتواند بیماری ایدز را به درستی تشخیص دهد و اکثر کیس ها را منفی در نظر می گیردtp\n",
    "در نتیجه دقت آن زیاد است اما از آن جایی که در مجموع در تشخیص دقت بالایی نداریم و کیس هایی که واقعا ایدز ندارند هم زیادند اما ما اکثر آن ها را گفتیم که ندارند بس دقت کمی داریم و عملکرد خوبی نداریم\n",
    "\n",
    "\n",
    "\n",
    "۴-\n",
    "اگر کلمه ای در متن خبر باشد که در کلمات یک کتگوری نباشد به جای احتمال آن :\n",
    "۱/تعداد کل کلمات کتگوری \n",
    "قرار میدهیم\n",
    "\n",
    "و اگر کلمه در یکی از دسته ها یک بار آمده باشد به جای احتمال آن \n",
    "۲/تعداد کل کلمات کتگوری \n",
    "قرار می دهیم\n",
    "\n",
    "بدین ترتیب در حالت اول لگاریتم احتمال عدد منفی کوچک تری نسبت به لگاریتم احتمال حالت دوم است. بنابراین در مجموع کتگوری ای که کلمه یک بار در آن آمده است احتمال بیشتری می گیرد و انتخاب شدن آن محتمل تر است (با توجه به احتمال دیگر کلمات خبر)و اگر احتمال بقیه کلمات خبر یکسان باشد بس حتما کتگوری شامل آن کلمه چون احتمال بیشتری دارد انتخاب می شود\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
